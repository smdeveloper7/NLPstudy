## :book: 워드 임베딩
텍스틀 컴퓨터가 이해하고, 효율적으로 처리하게 하기 위해서는 컴퓨터가 이해할 수 있도록 텍스트를 적절히 숫자로 변환해야합니다. 단어를 표현하는 방법에 따라서 자연어 처리의 성능이 크게 달라지기 때문에 단어를 수치화 하기 위한 많은 연구가 있었고, 현재에 이르러서는 각 단어를 인공 신경망 학습을 통해 벡터화 하는 워드 임베딩이라는 방법이 가장 많이 사용되고 있습니다.<br>

>[위키독스](https://wikidocs.net/22644)


---

### 1. 희소 표현(Sparse Representation)

원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표햔 방법이다.<br>
벡터나 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(Sparse representation)이라고 한다.<br>
원-핫 벡터는 희소 벡터(Sparse Vecotr)

:white_check_mark:


:x:*단점*
>코퍼스에 단어가 1000개였다면 벡터의 차원은 10,000 이다. 강아지란 단어의 인덱스가 4였다면 원 핫 벡터에서는 강아지 = [0 0 0 1 0 0 0 0 ]<br>

단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다. 공간적 낭비가 크다.

---

